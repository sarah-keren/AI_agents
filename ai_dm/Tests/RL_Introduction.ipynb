{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3XG5CToNcod"
      },
      "source": [
        "# Introduction to RL "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdAXMAIfRh9h"
      },
      "source": [
        "Based on the tutorial by https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adLPbDGVNiw-"
      },
      "source": [
        "For this tutorial we are going to use the Open-AI Taxi domain which is installed below (each code cell is executed by pressing shift+enter)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZvA_GvjDIui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2018eb8c-dbb4-40e4-f1e6-96b2faec8958"
      },
      "source": [
        "!pip install cmake 'gym[atari]' scipy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cmake in /usr/local/lib/python3.7/dist-packages (3.12.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (7.1.2)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.2.9)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari]) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKwFV_0oyFLa"
      },
      "source": [
        "import the taxi domain, and render an initial environment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u51aEt3HDYDb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "922aff31-ebdf-4901-cfe0-cfd456357634"
      },
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "\n",
        "env.reset ()\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cb5pEoyU8Lc"
      },
      "source": [
        "\n",
        "The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
        "The pipes (\"|\") represent walls which the taxi cannot traverse.\n",
        "R, G, Y, B are the possible pickup and destination locations. The passenger can also be in the taxi. The blue letter represents the current passenger pick-up location, and the purple letter is the current destination."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh4ywyvoViVI"
      },
      "source": [
        "The actions space is:\n",
        "\n",
        "0 = south\n",
        "1 = north\n",
        "2 = east\n",
        "3 = west\n",
        "4 = pickup\n",
        "5 = dropoff\n",
        "\n",
        "The state encoding is as follows (play around with the values)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvF8S_aLDaex",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "3a0b357b-1afd-423a-e5e0-1476430f5d8d"
      },
      "source": [
        "state = env.encode(1, 0, 2, 1) # (taxi row, taxi column, passenger index, destination index)\n",
        "print(\"State:\", state)\n",
        "\n",
        "env.render()\n",
        "env.s = state\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d8d8c5daed3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (taxi row, taxi column, passenger index, destination index)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"State:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmdH-CT6y71K"
      },
      "source": [
        "Another example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1IV5H9NnWnh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "417695e8-52a8-47c6-8ede-f823150ff827"
      },
      "source": [
        "state = env.encode(2, 3, 0, 1) # (taxi row, taxi column, passenger index, destination index)\n",
        "print(\"State:\", state)\n",
        "\n",
        "env.s = state\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: 261\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiQkecHBVZMY"
      },
      "source": [
        "P is the transition function, implementead as a dictionary with the structure {action: [(probability, nextstate, reward, done)]}.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIsXslpqWnUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caf712e4-6c24-48df-dff0-6913ebb58f2c"
      },
      "source": [
        "state = env.encode(0, 0, 0, 1) # (taxi row, taxi column, passenger index, destination index)\n",
        "env.s = state\n",
        "env.render()\n",
        "print('state:',state)\n",
        "\n",
        "\n",
        "print(env.P[state])\n",
        "# pickup the agent\n",
        "state, reward, done, info = env.step(0)\n",
        "state, reward, done, info = env.step(0)\n",
        "state, reward, done, info = env.step(2)\n",
        "env.s = state\n",
        "env.render()\n",
        "\n",
        "print('state:',state)\n",
        "print('reward:', reward)\n",
        "print('done:', done)\n",
        "print('info:', info)\n",
        "print('transition func:', env.P[state])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "\n",
            "state: 1\n",
            "{0: [(1.0, 101, -1, False)], 1: [(1.0, 1, -1, False)], 2: [(1.0, 21, -1, False)], 3: [(1.0, 1, -1, False)], 4: [(1.0, 17, -1, False)], 5: [(1.0, 1, -10, False)]}\n",
            "+---------+\n",
            "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (East)\n",
            "state: 221\n",
            "reward: -1\n",
            "done: False\n",
            "info: {'prob': 1.0}\n",
            "transition func: {0: [(1.0, 321, -1, False)], 1: [(1.0, 121, -1, False)], 2: [(1.0, 241, -1, False)], 3: [(1.0, 201, -1, False)], 4: [(1.0, 221, -10, False)], 5: [(1.0, 221, -10, False)]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgogXvAvNyth"
      },
      "source": [
        "# Brute Force Approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpHC2AcEWW-q"
      },
      "source": [
        "Let's see what would happen if we try to brute-force our way to solving the problem without RL.\n",
        "\n",
        "Since we have our P table for default rewards in each state, we can try to have our taxi navigate just using that.\n",
        "\n",
        "We'll create an infinite loop which runs until one passenger reaches one destination (one episode), or in other words, when the received reward is 20. The env.action_space.sample() method automatically selects one random action from set of all possible actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5LGYQA6FCkj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "194c136a-8646-4602-cbc2-730903701072"
      },
      "source": [
        "env.s = 328  # set environment to illustration's state\n",
        "\n",
        "epochs = 0\n",
        "penalties, reward = 0, 0\n",
        "\n",
        "frames = [] # for animation\n",
        "\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, done, info = env.step(action)\n",
        "\n",
        "    if reward == -10:\n",
        "        penalties += 1\n",
        "    \n",
        "    # Put each rendered frame into dict for animation\n",
        "    frames.append({\n",
        "        'frame': env.render(mode='ansi'),\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "        }\n",
        "    )\n",
        "\n",
        "    epochs += 1\n",
        "    \n",
        "    \n",
        "print(\"Timesteps taken: {}\".format(epochs))\n",
        "print(\"Penalties incurred: {}\".format(penalties))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timesteps taken: 1099\n",
            "Penalties incurred: 347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZT78EsuaqsQ"
      },
      "source": [
        "Play the animation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fehEet45FvwC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "e40228e3-de6e-460f-9aa1-66540c714c59"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.1)\n",
        "        \n",
        "print_frames(frames)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (East)\n",
            "\n",
            "Timestep: 454\n",
            "State: 388\n",
            "Action: 2\n",
            "Reward: -1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-adcf58a03b7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-adcf58a03b7e>\u001b[0m in \u001b[0;36mprint_frames\u001b[0;34m(frames)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Action: {frame['action']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Reward: {frame['reward']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ih4KgkPN3SC"
      },
      "source": [
        "# Q Learning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa_hT2rMnkT_"
      },
      "source": [
        "Essentially, Q-learning lets the agent use the environment's rewards to learn, over time, the best action to take in a given state.\n",
        "\n",
        "In our Taxi environment, we have the reward table, P, that the agent will learn from. It does thing by looking receiving a reward for taking an action in the current state, then updating a Q-value to remember if that action was beneficial.\n",
        "\n",
        "The values store in the Q-table are called a Q-values, and they map to a (state, action) combination.\n",
        "\n",
        "A Q-value for a particular state-action combination is representative of the \"quality\" of an action taken from that state. Better Q-values imply better chances of getting greater rewards.\n",
        "\n",
        "For example, if the taxi is faced with a state that includes a passenger at its current location, it is highly likely that the Q-value for pickup is higher when compared to other actions, like dropoff or north.\n",
        "\n",
        "Q-values are initialized to an arbitrary value, and as the agent exposes itself to the environment and receives different rewards by executing different actions, the Q-values are updated using the equation:\n",
        "\n",
        "\n",
        "Q(state,action)←(1−α)Q(state,action)+α(reward+γmaxaQ(next state,all actions))\n",
        "Where:\n",
        "\n",
        "- α (alpha) is the learning rate (0<α≤1) - Just like in supervised learning settings, α is the extent to which our Q-values are being updated in every iteration.\n",
        "\n",
        "- γ (gamma) is the discount factor (0≤γ≤1) - determines how much importance we want to give to future rewards. A high value for the discount factor (close to 1) captures the long-term effective award, whereas, a discount factor of 0 makes our agent consider only immediate reward, hence making it greedy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CmPXRImfX67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4db2cc60-396a-465c-9f4b-c9370a5b8eb9"
      },
      "source": [
        "print(env.observation_space)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7PAhXaUF7p7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1a617b4-2536-45d2-a74a-ebd3126637f3"
      },
      "source": [
        "#Creating the q_table\n",
        "import numpy as np\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "print(q_table)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wttjDUyqIbsJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d754c0c-808c-4387-b7e1-9a9d02487f34"
      },
      "source": [
        "%%time\n",
        "\"\"\"Training the agent\"\"\"\n",
        "\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1 # the learning rate\n",
        "gamma = 0.6 # discount factor\n",
        "epsilon = 0.1 # taking random actions\n",
        "\n",
        "# For plotting metrics\n",
        "all_epochs = []\n",
        "all_penalties = []\n",
        "\n",
        "for i in range(1, 100001):\n",
        "    state = env.reset()\n",
        "\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample() # Explore action space\n",
        "        else:\n",
        "            action = np.argmax(q_table[state]) # Exploit learned values\n",
        "\n",
        "        next_state, reward, done, info = env.step(action) \n",
        "        \n",
        "        old_value = q_table[state, action]\n",
        "        next_max = np.max(q_table[next_state])\n",
        "        \n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "        \n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episode: {i}\")\n",
        "\n",
        "print(\"Training finished.\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n",
            "CPU times: user 1min 8s, sys: 15.6 s, total: 1min 24s\n",
            "Wall time: 1min 11s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llW9EYInPxTV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "591d1ca7-e7dd-4214-a33e-f6790388150f"
      },
      "source": [
        "print(q_table[361])\n",
        "print(q_table[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ -2.49496543  -2.48942084  -2.49553924  -2.49335055 -11.18795654\n",
            " -10.77390738]\n",
            "[ -2.41837063  -2.3639511   -2.41837062  -2.36395109  -2.27325184\n",
            " -11.36395103]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPCQyYDBetZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3caec485-0f3e-4a60-eeb2-20043ea4b5f9"
      },
      "source": [
        "  state = env.reset()\n",
        "  env.render()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTxDB849N81z"
      },
      "source": [
        "# Evaluating the performance of the Q-Learning approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XuPzRfhIvKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "051b1b30-d2e7-4ab9-f768-0f8ea2933250"
      },
      "source": [
        "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
        "\n",
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 100\n",
        "\n",
        "frames = []\n",
        "\n",
        "for _ in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    \n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state])\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        frames.append({\n",
        "          'frame': env.render(mode='ansi'),\n",
        "          'state': state,\n",
        "          'action': action,\n",
        "          'reward': reward\n",
        "          }\n",
        "        )\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 12.88\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_78V3yIPg8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6e31298-0b6d-411a-b2d4-dbe1c1c6d407"
      },
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.5)\n",
        "        \n",
        "print_frames(frames)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Timestep: 1288\n",
            "State: 475\n",
            "Action: 5\n",
            "Reward: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ins1Q7CYoDpx"
      },
      "source": [
        "Q-learning is one of the easiest Reinforcement Learning algorithms. The problem with Q-earning however is, once the number of states in the environment are very high, it becomes difficult to implement them with Q table as the size would become very, very large. State of the art techniques uses Deep neural networks instead of the Q-table (Deep Reinforcement Learning). The neural network takes in state information and actions to the input layer and learns to output the right action over the time. Deep learning techniques (like Convolutional Neural Networks) are also used to interpret the pixels on the screen and extract information out of the game (like scores), and then letting the agent control the game."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vc-Folk7oEj5"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}